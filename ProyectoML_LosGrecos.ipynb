{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TeJH_a1_AIYF"
   },
   "source": [
    "# Introduction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bz9xzrNH3BgK"
   },
   "source": [
    "In this project we aim to model a primary tumor classifier in order to apply what we have learnt at Artificial Intelligence and Learning, a subject that is taught at the Biomedical Engineering degree (Rey Juan Carlos University). \n",
    "\n",
    "We proceed to analyze the dataset we are going to work with hereunder.\n",
    "\n",
    "The dataset we have chosen is a very known one about **primary tumors**. This database is mainly composed of **categorical multivariable features**. As a result, this is a **classification** problem. \n",
    "\n",
    "The **total number of patients** we have is **339** where as the **number of features** is **18**. We will have to deal with a total of 225 missing values.\n",
    "\n",
    "This dataset was created at November 1st 1988 by the **Oncology institut** of the universitary medical center of **Ljubljana, Slovenia**.\n",
    "\n",
    "The **features** we are going to work with are:   \n",
    "    -Tumor's class   \n",
    "    -Patient's age    \n",
    "    -Patient's gender    \n",
    "    -Histological type of the tumor    \n",
    "    -Degree of diferentiation (simmilarity with respect to the neighbouring cells)   \n",
    "    -Starting place (bone, pleura, bone marrow, lung, lung pleura, peritoneum, liver, brain, skin, supraclavicular region, axilar region, mediastinum or abdominal region).   \n",
    "\n",
    "A **primary tumor** is a tumor that grows on the anatomical place where the progression started; this tumor creates a cancerous mass (no methastasis). *This is the main difference when compared with secondary tumors*, which are the ones that get distributed throughout the rest of the body.\n",
    "\n",
    "\n",
    "The distribution on our 22 classes is not uniform; we have an imbalanced dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CU-wraKlAAuM"
   },
   "source": [
    "# Analysis code: Data visualization.\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We proceed to read the dataset in order to see its aspect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "dti1x5BCAPkA"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'primary-tumor.data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mskm\u001b[39;00m\n\u001b[1;32m      4\u001b[0m columnas \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msex\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhistologic-type\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdegree-of-diffe\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbone\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbone-marrow\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlung\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpleura\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mperitoneum\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mliver\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbrain\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mskin\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mneck\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupraclavicular\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124maxillar\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmediastinum\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabdominal\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 5\u001b[0m file \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprimary-tumor.data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnames\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcolumnas\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Proyectos/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Proyectos/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Proyectos/lib/python3.10/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Proyectos/lib/python3.10/site-packages/pandas/io/parsers/readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/anaconda3/envs/Proyectos/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Proyectos/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1735\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1734\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1736\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1737\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1738\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1739\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1740\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1741\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1742\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1743\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1744\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/anaconda3/envs/Proyectos/lib/python3.10/site-packages/pandas/io/common.py:856\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    852\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    853\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    854\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    855\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 856\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    865\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'primary-tumor.data'"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import statistics as stats\n",
    "import sklearn.model_selection as skm\n",
    "columnas = ['class', 'age', 'sex', 'histologic-type', 'degree-of-diffe', 'bone', 'bone-marrow', 'lung', 'pleura', 'peritoneum', 'liver', 'brain', 'skin', 'neck', 'supraclavicular', 'axillar', 'mediastinum', 'abdominal']\n",
    "file = pd.read_csv('primary-tumor.data', sep=',', names = columnas) \n",
    "#We have to change the name of features as it was originally changed to numbers; the idea\n",
    "#is to improve the interpretability of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5soCtBSpJiul"
   },
   "source": [
    " # Preprocessing code: Integration, cleaning, transformation and data reduction. \n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iaTUfkTz21GF"
   },
   "source": [
    "We proceed hereunder to process the data we have in order to work with the optimal features of our data and get the best results on classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N2zwslxL3ADY",
    "outputId": "3d29dc16-0794-4669-c361-6ef0018a866b"
   },
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "import matplotlib.pyplot as plt\n",
    "import statistics as stat\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import scipy as sc\n",
    "import random\n",
    "!pip install miceforest \n",
    "import miceforest as mf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yUKg9zlcBwvp"
   },
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first approximation we took was to try to impute the best value for each cancer [1],[2],[3],[4],[5]. \n",
    "Afterwards, several type of techniques were investigated to delete NaN values[6], [7].   \n",
    "\n",
    "We decided finally to use **MICE (multiple imputation by chained equations)** algorithm to deal with missing values. \n",
    "\n",
    "For those features with a single NaN value, we will assume it was lost by random (*Missing Completely at Random*).   \n",
    "Otherwise, as we see a relatively higher percentage of missing values on rather important features (according to the literature), we opt to impute them instead of deleting them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Y_k4svqDBl1u",
    "outputId": "bbb66c4b-3da4-42a7-dd6d-75b32387bff5"
   },
   "outputs": [],
   "source": [
    "#We change the ? values for NaN.\n",
    "file = file.replace('?', np.nan)\n",
    "\n",
    "#MICE's kernel is created, using 4 datasets. \n",
    "#Firstly, we convert all the features into categorical [8]\n",
    "for col in columnas:\n",
    "    file[col] = file[col].astype('category',copy=False)\n",
    "kernel = mf.ImputationKernel(\n",
    "  file,\n",
    "  save_all_iterations=True,\n",
    "  random_state=1\n",
    ")\n",
    "kernel.mice(3)\n",
    "completed_dataset = kernel.complete_data(dataset=0, inplace=False) #our new imputed dataset\n",
    "print(completed_dataset.isnull().sum(0))\n",
    "\n",
    "#kernel.plot_imputed_distributions(wspace=0.35,hspace=0.4)\n",
    "completed_dataset.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d_UmPfS9B35N"
   },
   "source": [
    "Transformación de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 731
    },
    "id": "nW8j304g4Tj6",
    "outputId": "189071c5-40c8-4168-c17e-5dfe354d0154"
   },
   "outputs": [],
   "source": [
    "#Dado que no tener los datos en una base 0/1/2... puede afectar a los valores de ciertas correlaciones, \n",
    "#(método de Kendall, que usaremos más adelante) pasamos a transformar los datos para obtener el resultado más preciso posible.\n",
    "\n",
    "clase=completed_dataset[\"class\"]\n",
    "input = completed_dataset.drop(columns='class')\n",
    "columna_sin_clase = ['age', 'sex', 'histologic-type', 'degree-of-diffe', 'bone', 'bone-marrow', 'lung', 'pleura', 'peritoneum', 'liver', 'brain', 'skin', 'neck', 'supraclavicular', 'axillar', 'mediastinum', 'abdominal']\n",
    "for col in columna_sin_clase:\n",
    "    input[col] = input[col].astype('int64',copy=False)\n",
    "input = input.replace(to_replace = 1, value = 0, regex = True)\n",
    "input = input.replace(to_replace = 2, value = 1, regex = True)\n",
    "input = input.replace(to_replace = 3, value = 2, regex = True)\n",
    "\n",
    "completed_dataset=input\n",
    "completed_dataset[\"class\"]=clase\n",
    "\n",
    "input.head(20)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "WdA7dBk5Ees1",
    "outputId": "fac53487-26d1-4521-b33b-bd8d80c6abd2"
   },
   "outputs": [],
   "source": [
    "completed_dataset.hist(figsize=(12, 12), bins=3, xlabelsize=8, ylabelsize=8) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the features are imbalanced, that is, we have categories inside our feature that have much more values than the rest. When we study our classes, we will see the same. We will assume the cost of this imbalance on precission and r2_score; nevertheless, in future versions of this project, we aim to balance properly this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "id": "bOEKSoI4Ej1W",
    "outputId": "56bd28f4-e86f-447c-d2ef-abfb8628661f"
   },
   "outputs": [],
   "source": [
    "completed_dataset[\"class\"].hist(figsize=(3, 6), bins=13, xlabelsize=8, ylabelsize=8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tM4OkvAJCFfM"
   },
   "source": [
    "# Data reduction Phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IjMcsayOObTn",
    "outputId": "14842b8b-cf37-4765-8000-4718fabe363d"
   },
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "#We will calculate Cramer's Phi statistic, which is a variant of Student's T for ordinal discrete features.\n",
    "X2 = stats.chi2_contingency(input, correction=False)[0]\n",
    "n = np.sum(input)\n",
    "minDim = min(input.shape)-1\n",
    "\n",
    "\n",
    "V = np.sqrt((X2/n) / minDim)\n",
    "\n",
    "print(V)\n",
    "\n",
    "#The idea is to complement the Kendall data with Cramer data related to correlation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 591
    },
    "id": "pqQ0LceJxBCs",
    "outputId": "0d80d757-67f3-4928-9d50-2c70ee56a91a"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,12))\n",
    "sns.heatmap(input.corr(method = 'kendall'),linewidths=.1,cmap=\"YlGnBu\", annot=True)\n",
    "plt.yticks(rotation=0);\n",
    "\n",
    "#As we can see hereunder, we do not have very correlated features. Kendall is more robust than Cramer's Phi\n",
    "#because categorical features rarely follow a normal distribution (an assumption taken for Cramer's Phi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To end this project, we see that, for several classes, the number of features is quite low. To solve this problem, we proceed to delete this \"outliers\" of the features and group those who are more similar (according to the state of the art on primary tumors). \n",
    "\n",
    "We proceed to delete feature number 9, merge features 19, 20, 18 on \"utherus cancer\" [11] .\n",
    "\n",
    "We will delete vagina cancer and testicles cancer and merge colon cancer and rectus cancer on \"colorrectal cancer\"[12]. We will merge bladder cancer and prostatic cancer on bladder or prosthatic cancer [13]; we will, finally, delete duodenal cancer and salivary cancer as they are rarer in prevalence.\n",
    "\n",
    "Our main idea is to avoid that logistic regression always chooses the most represented class (one of its best known biases) and delete those cancers that are never going to be chosen as a result of their lower probability.\n",
    "\n",
    "Despite our database is going to be imbalanced, at this moment is going to be less imbalanced than before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7glyB4adJSuQ"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "completed_dataset[completed_dataset[\"class\"] == 9]\n",
    "\n",
    "completed_dataset[completed_dataset['class'] == 20] \n",
    "completed_dataset.at[313, 'class'] = 18\n",
    "completed_dataset.at[312, 'class'] = 18\n",
    "completed_dataset[completed_dataset['class'] == 19]\n",
    "lista = [306, 307, 308, 309, 310, 311]\n",
    "for i in range(len(lista)):\n",
    "  completed_dataset.at[lista[i], 'class'] = 18\n",
    "completed_dataset[completed_dataset['class'] == 21] \n",
    "\n",
    "completed_dataset.drop(314,axis=0, inplace=True) \n",
    "\n",
    "completed_dataset[completed_dataset['class'] == 16] \n",
    "\n",
    "completed_dataset.drop(266, axis = 0, inplace=True)\n",
    "completed_dataset[completed_dataset['class'] == 8] \n",
    "\n",
    "lista = [181, 182, 183, 184, 185, 186]\n",
    "for i in range(len(lista)):\n",
    "  completed_dataset.at[lista[i], 'class'] = 7\n",
    "completed_dataset[completed_dataset['class'] == 15]  \n",
    "\n",
    "lista = [264, 265]\n",
    "for i in range(len(lista)):\n",
    "  completed_dataset.at[lista[i], 'class'] = 17\n",
    "\n",
    "completed_dataset.drop(166, axis = 0, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "completed_dataset.drop(187, axis = 0, inplace=True)\n",
    "completed_dataset.drop(188, axis = 0, inplace=True)\n",
    "\n",
    "output = completed_dataset['class']\n",
    "input = completed_dataset.drop(columns='class')\n",
    "output.cat.remove_categories(removals = [6, 10, 20, 19, 21, 16, 8, 15], inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MIRY_XIl5c3t"
   },
   "source": [
    "Bibliografía:\n",
    "1. Li, X., Xu, H., Yan, L., Gao, J., & Zhu, L. (2021). A Novel Clinical Nomogram for Predicting Cancer-Specific Survival in Adult Patients After Primary Surgery for Epithelial Ovarian Cancer: A Real-World Analysis Based on the Surveillance, Epidemiology, and End Results Database and External Validation in a Tertiary Center. Frontiers in oncology, 11, 670644. https://doi.org/10.3389/fonc.2021.670644\n",
    "2. Lee, C. C., Ho, H. C., Su, Y. C., Yu, C. H., & Yang, C. C. (2015). Modified Tumor Classification With Inclusion of Tumor Characteristics Improves Discrimination and Prediction Accuracy in Oral and Hypopharyngeal Cancer Patients Who Underwent Surgery. Medicine, 94(27), e1114. https://doi.org/10.1097/MD.0000000000001114\n",
    "3. Schlageter, M., Terracciano, L. M., D'Angelo, S., & Sorrentino, P. (2014). Histopathology of hepatocellular carcinoma. World journal of gastroenterology, 20(43), 15955–15964. https://doi.org/10.3748/wjg.v20.i43.15955\n",
    "4. Caratozzolo E, Massani M, Recordare A, Ciardo L, Antoniutti M, Jelmoni A, Bassi N. Squamous cell liver cancer arising from an epidermoid cyst. J Hepatobiliary Pancreat Surg. 2001;8(5):490-3. doi: 10.1007/s005340100015. PMID: 11702262.\n",
    "5. Dhasade, G. (2020, November 14). Ways To Handle Categorical Column Missing Data & Its Implementations. Medium. https://medium.com/analytics-vidhya/ways-to-handle-categorical-column-missing-data-its-implementations-15dc4a56893\n",
    "6. Azur, M. J., Stuart, E. A., Frangakis, C., & Leaf, P. J. (2011). Multiple imputation by chained equations: what is it and how does it work?. International journal of methods in psychiatric research, 20(1), 40–49. https://doi.org/10.1002/mpr.329\n",
    "7. Dolladille, C., Chrétien, B., Peyro-Saint-Paul, L. et al. Association Between Disease-Modifying Therapies Prescribed to Persons with Multiple Sclerosis and Cancer: a WHO Pharmacovigilance Database Analysis. Neurotherapeutics (2021). https://doi.org/10.1007/s13311-021-01073-y\n",
    "8. miceforest. (2021, October 16). PyPI. https://pypi.org/project/miceforest/\n",
    "9. Kinoshita Y, Singh A, Rovito PM Jr, Wang CY, Haas GP. Double primary cancers of the prostate and bladder: a literature review. Clin Prostate Cancer. 2004 Sep;3(2):83-6. doi: 10.3816/cgc.2004.n.016. PMID: 15479490.\n",
    "10. Griffiths, C. D., McKechnie, T., Lee, Y., Springer, J. E., Doumouras, A. G., Hong, D., & Eskicioglu, C. (2021). Presentation and survival among patients with colorectal cancer before the age of screening: a systematic review and meta-analysis. Canadian journal of surgery. Journal canadien de chirurgie, 64(1), E91–E100. https://doi.org/10.1503/cjs.013019 \n",
    "11. Wu, M., Gunning, W., & Ratnam, M. (1999, September). Expression of Folate Receptor Type α in Relation to Cell Type, Malignancy, and Differentiation in Ovary, Uterus, and Cervix. Cancer Epidemiology, Biomarkers and Prevention. Published. https://cebp.aacrjournals.org/content/8/9/775\n",
    "12. Siegel, R.L., Miller, K.D., Goding Sauer, A., Fedewa, S.A., Butterly, L.F., Anderson, J.C., Cercek, A., Smith, R.A. and Jemal, A. (2020), Colorectal cancer statistics, 2020. CA A Cancer J Clin, 70: 145-164. https://doi.org/10.3322/caac.21601\n",
    "13. Chun, T. Y. (1997). Coincidence of bladder and prostate cancer. Journal of Urology. Published. https://doi.org/10.1016/s0022-5347(01)65281-9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "swKptuVJHo96"
   },
   "source": [
    "# Classification and regression using synthetic features: linear and non-linear "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To show our knowledge about continuous features and given that our entire dataset is categorical, we will add a synthetic random variable named Proba-of-rec that will use numerical data to show the \"probability of recuperation from cancer\". Given that is a synthetic variable, we do not recommend to obtain conclusions from these results in a clinical context. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sa02nrMJ-Cxt",
    "outputId": "900ec4a8-277e-40ff-f0e6-63a63a902e92"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "input_lr = input\n",
    "columna_sin_clase = ['age', 'sex', 'histologic-type', 'degree-of-diffe', 'bone', 'bone-marrow', 'lung', 'pleura', 'peritoneum', 'liver', 'brain', 'skin', 'neck', 'supraclavicular', 'axillar', 'mediastinum', 'abdominal', 'proba-of-rec']\n",
    "input_lr['proba-of-rec'] = np.random.randint(0,  100, size=len(input[\"age\"]))\n",
    "\n",
    "#Dividing our dataset in train and test.\n",
    "\n",
    "X_train_lr, X_test_lr, Y_train_lr, Y_test_lr = train_test_split(input_lr, output, test_size = 0.3, random_state = 42)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "skf.get_n_splits(X_train_lr, Y_train_lr)\n",
    "\n",
    "train = []\n",
    "validation = []\n",
    "for train_index, validation_index in skf.split(X_train_lr, Y_train_lr):\n",
    "  train_groups= []\n",
    "  validation_groups= []\n",
    "  train_groups.append(train_index)\n",
    "  validation_groups.append(validation_index)\n",
    "  train.append(train_groups)\n",
    "  validation.append(validation_groups)\n",
    "\n",
    "\n",
    "#We will assume that our data is a iid variable (identically distributed and independent); it is not a temporal series.\n",
    "#This could be a limitation of our study that we are assuming for simplicity. [14]\n",
    "\n",
    "\n",
    "regressor = LinearRegression()\n",
    "scores_ecm = cross_val_score(regressor, X_train_lr, Y_train_lr, cv=skf, scoring = 'neg_mean_squared_error', n_jobs = -1)\n",
    "scores_r2 = cross_val_score(regressor, X_train_lr, Y_train_lr, cv=skf, scoring = 'r2', n_jobs = -1)\n",
    "\n",
    "\n",
    "print(\"ECM Scores: {}\".format(scores_ecm))\n",
    "print('R2 Scores : {}'.format(scores_r2))\n",
    "print(np.mean(np.absolute(scores_ecm)))\n",
    "print(np.mean(np.absolute(scores_r2)))\n",
    "\n",
    "\n",
    "regressor = regressor.fit(X_train_lr,Y_train_lr) \n",
    "y_predt = regressor.predict(X_test_lr)\n",
    "\n",
    "mse=mean_squared_error(Y_test_lr, y_predt)\n",
    "\n",
    "print(mse)\n",
    "\n",
    "r_squared=r2_score(Y_test_lr, y_predt)\n",
    "print(r_squared)\n",
    "\n",
    "#It is very probable that our bad results on r_squared are due to the synthetic variable and the use of linear regression\n",
    "#on a mixed dataset; perhaps it would have been more useful to use decision trees or random forest, which \n",
    "#are rather more robust in this type of problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "syKS1o79Lwl3"
   },
   "source": [
    "14. (2021, May 1). Multiple Linear Regression using Python and Scikit Learn.  https://www.analyticsvidhya.com/blog/2021/05/multiple-linear-regression-using-python-and-scikit-learn/\n",
    "15. (2020, August 19). Why and how to Cross Validate a Model? - Towards Data Science. Medium. https://towardsdatascience.com/why-and-how-to-cross-validate-a-model-d6424b45261f?gi=c3c1a4538a47\n",
    "16. (2020, March 13). The Importance Of Cross Validation In Machine Learning. Digital Vidya. https://www.digitalvidya.com/blog/cross-validation-in-machine-learning/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to try to figure out in this cell to see which may be the best hyperparameter to see the relation\n",
    "with the probability of recovering from cancer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature         | MSE     | R2 |\n",
    "|--------------|-----------|------------|\n",
    "| Age | 58      |   -0.003     |\n",
    "| Sex      | 50.52  |    0.125    |\n",
    "| Histologic-type | 55 | 0.48 |\n",
    "| Degree-of-diffe | 56.56 | 0.0213 |\n",
    "| Bone | 57.47 | 0.005675 |\n",
    "| Bone-marrow | 58.13 | -0.00577 |\n",
    "|  Lung | 57.61 | 0.003 |\n",
    "| Pleura | 57.85 | -0.0008 |\n",
    "| Peritoneum | 55.59 | 0.038 |\n",
    "| Liver | 58.97 | -0.02 |\n",
    "| Brain | 54.395 | 0.05895 |\n",
    "| Skin | 58.13 | -0.006 |\n",
    "| Neck | 58.13 | -0.005 |\n",
    "| Supraclavicular | 56.63 | 0.02 |\n",
    "| Axillar | 50.91 | 0.119 |\n",
    "| Mediastinum | 55.9 | 0.03 |\n",
    "| Abdominal | 57.083 | 0.012 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it can be seen, the predictive performance of our algorithm taking into account the synthetical variable is worse given the randomness of the variable and its comparison with variables that are completely categorical. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rRPnNeOk-8gF",
    "outputId": "e659e06b-c3e9-4ae4-b6a4-0e65bfdcfe4e"
   },
   "outputs": [],
   "source": [
    "lista = [X_train_lr['proba-of-rec'], X_train_lr['sex']]\n",
    "prueba = pd.concat(lista, axis=1)\n",
    "\n",
    "lista1 = [X_test_lr['proba-of-rec'], X_test_lr['sex']]\n",
    "prueba2 = pd.concat(lista1, axis = 1)\n",
    "\n",
    "regressor = regressor.fit(prueba,Y_train_lr) \n",
    "y_pred = regressor.predict(prueba2)\n",
    "\n",
    "# Calculamos el error cuadrático medio\n",
    "mse=mean_squared_error(Y_test_lr, y_pred)\n",
    "\n",
    "print(mse)\n",
    "\n",
    "# Calculamos R^2\n",
    "r_squared=r2_score(Y_test_lr, y_pred)\n",
    "print(r_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "id": "xJH8hnStD-Af",
    "outputId": "825ac1ea-7c7a-406f-c549-fedd874a05e4"
   },
   "outputs": [],
   "source": [
    "sns.regplot(y_predt, Y_test_lr)\n",
    "plt.xlabel('y_pred')\n",
    "plt.ylabel('y_test')\n",
    "plt.show()\n",
    "#Como podemos ver, obedece mejor a una sigmoide que a una recta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PII60cDJ-iwO"
   },
   "source": [
    "As it may be seen, given the sample distribution, the modelling of the decision frontier is much more efficient by using models such as Logistic Regression. \n",
    "\n",
    "Hereunder, we proceed to add another type of regressions based on Ridge, Lasso and Elastic Net to see how they may be better to improve the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "77NO_cWgL5gD",
    "outputId": "73ccca44-bb5b-452c-cdf2-26e9722576c2"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "regressor = Lasso()\n",
    "scores_ecm = cross_val_score(regressor, X_train_lr, Y_train_lr, cv=skf, scoring = 'neg_mean_squared_error', n_jobs = -1)\n",
    "scores_r2 = cross_val_score(regressor, X_train_lr, Y_train_lr, cv=skf, scoring = 'r2', n_jobs = -1)\n",
    "\n",
    "\n",
    "print(\"ECM Scores: {}\".format(scores_ecm))\n",
    "print('R2 Scores : {}'.format(scores_r2))\n",
    "print(np.mean(np.absolute(scores_ecm)))\n",
    "print(np.mean(np.absolute(scores_r2)))\n",
    "\n",
    "\n",
    "\n",
    "regressor = regressor.fit(X_train_lr,Y_train_lr) \n",
    "y_predt = regressor.predict(X_test_lr)\n",
    "\n",
    "\n",
    "mse=mean_squared_error(Y_test_lr, y_pred)\n",
    "\n",
    "print(mse)\n",
    "\n",
    "# Calculamos R^2\n",
    "r_squared=r2_score(Y_test_lr, y_pred)\n",
    "print(r_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "id": "ZdrFc7kkMqat",
    "outputId": "791a0b4b-6af1-44e1-aa66-4d3c9745a3d0"
   },
   "outputs": [],
   "source": [
    "sns.regplot(y_predt, Y_test_lr)\n",
    "plt.xlabel('y_pred')\n",
    "plt.xlim(0, 20)\n",
    "plt.ylabel('y_test')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GQzn-YJ6NBrU",
    "outputId": "0015f8ac-d247-459b-e69d-ebceb3ee1104"
   },
   "outputs": [],
   "source": [
    "ridge = Ridge()\n",
    "scores_ecm = cross_val_score(ridge, X_train_lr, Y_train_lr, cv=skf, scoring = 'neg_mean_squared_error', n_jobs = -1)\n",
    "scores_r2 = cross_val_score(ridge, X_train_lr, Y_train_lr, cv=skf, scoring = 'r2', n_jobs = -1)\n",
    "\n",
    "\n",
    "print(\"ECM Scores: {}\".format(scores_ecm))\n",
    "print('R2 Scores : {}'.format(scores_r2))\n",
    "print(np.mean(np.absolute(scores_ecm)))\n",
    "print(np.mean(np.absolute(scores_r2)))\n",
    "\n",
    "\n",
    "\n",
    "regressor = ridge.fit(X_train_lr,Y_train_lr) \n",
    "y_predt = regressor.predict(X_test_lr)\n",
    "\n",
    "mse=mean_squared_error(Y_test_lr, y_pred)\n",
    "\n",
    "print(mse)\n",
    "\n",
    "r_squared=r2_score(Y_test_lr, y_pred)\n",
    "print(r_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qg4_r66ENRv_",
    "outputId": "c2b1a816-7bf4-4f44-aa38-f3aa96a6b3e5"
   },
   "outputs": [],
   "source": [
    "lista = [X_train_lr['proba-of-rec'], X_train_lr['sex']]\n",
    "prueba = pd.concat(lista, axis=1)\n",
    "\n",
    "lista1 = [X_test_lr['proba-of-rec'], X_test_lr['sex']]\n",
    "prueba2 = pd.concat(lista1, axis = 1)\n",
    "\n",
    "regressor = ridge.fit(prueba,Y_train_lr) \n",
    "y_pred = regressor.predict(prueba2)\n",
    "\n",
    "mse=mean_squared_error(Y_test_lr, y_pred)\n",
    "\n",
    "print(mse)\n",
    "\n",
    "r_squared=r2_score(Y_test_lr, y_pred)\n",
    "print(r_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "id": "njJZC04dNR2n",
    "outputId": "0e54de38-bae8-4532-ae96-bde498e67c46"
   },
   "outputs": [],
   "source": [
    "sns.regplot(y_predt, Y_test_lr)\n",
    "plt.xlabel('y_pred')\n",
    "plt.ylabel('y_test')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5e2cB8zlNkmF",
    "outputId": "d4c05fc5-3feb-4365-fb6a-7c1447cadc3e"
   },
   "outputs": [],
   "source": [
    "elasticnet = ElasticNet()\n",
    "scores_ecm = cross_val_score(elasticnet, X_train_lr, Y_train_lr, cv=skf, scoring = 'neg_mean_squared_error', n_jobs = -1)\n",
    "scores_r2 = cross_val_score(elasticnet, X_train_lr, Y_train_lr, cv=skf, scoring = 'r2', n_jobs = -1)\n",
    "\n",
    "\n",
    "print(\"ECM Scores: {}\".format(scores_ecm))\n",
    "print('R2 Scores : {}'.format(scores_r2))\n",
    "print(np.mean(np.absolute(scores_ecm)))\n",
    "print(np.mean(np.absolute(scores_r2)))\n",
    "\n",
    "\n",
    "\n",
    "regressor = elasticnet.fit(X_train_lr,Y_train_lr) \n",
    "y_predt = regressor.predict(X_test_lr)\n",
    "\n",
    "\n",
    "mse=mean_squared_error(Y_test_lr, y_pred)\n",
    "\n",
    "print(mse)\n",
    "\n",
    "\n",
    "r_squared=r2_score(Y_test_lr, y_pred)\n",
    "print(r_squared)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5h3Rbng1NykQ",
    "outputId": "322221a6-38fd-4481-f608-69fbe7fa4f98"
   },
   "outputs": [],
   "source": [
    "lista = [X_train_lr['proba-of-rec'], X_train_lr['sex']]\n",
    "prueba = pd.concat(lista, axis=1)\n",
    "\n",
    "lista1 = [X_test_lr['proba-of-rec'], X_test_lr['sex']]\n",
    "prueba2 = pd.concat(lista1, axis = 1)\n",
    "\n",
    "regressor = elasticnet.fit(prueba,Y_train_lr) \n",
    "y_pred = regressor.predict(prueba2)\n",
    "\n",
    "\n",
    "mse=mean_squared_error(Y_test_lr, y_pred)\n",
    "\n",
    "print(mse)\n",
    "\n",
    "\n",
    "r_squared=r2_score(Y_test_lr, y_pred)\n",
    "print(r_squared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "id": "gEWFRvuONyrH",
    "outputId": "c36eabca-1074-40ac-b8e1-c865f0b58ce9"
   },
   "outputs": [],
   "source": [
    "sns.regplot(y_predt, Y_test_lr)\n",
    "plt.xlabel('y_pred')\n",
    "plt.ylabel('y_test')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 630
    },
    "id": "s9fKrT-0tLiT",
    "outputId": "432df896-68e9-4a96-a3aa-cc875953f9ef"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "output = completed_dataset['class']\n",
    "input = completed_dataset.drop(columns='class')\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(input, output, test_size = 0.2, random_state = 42)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "skf.get_n_splits(X_train_lr, Y_train_lr)\n",
    "train = []\n",
    "validation = []\n",
    "\n",
    "for train_index, validation_index in skf.split(X_train, Y_train):\n",
    "    train_groups= []\n",
    "    validation_groups= []\n",
    "    train_groups.append(train_index)\n",
    "    validation_groups.append(validation_index)\n",
    "    train.append(train_groups)\n",
    "    validation.append(validation_groups)\n",
    "\n",
    "\n",
    "regressor = LogisticRegression(multi_class = 'multinomial', max_iter=1000, solver = 'saga' , penalty = 'none', n_jobs = -1)\n",
    "scores_ecm = cross_val_score(regressor, X_train, Y_train, cv=skf, scoring = 'neg_mean_squared_error', n_jobs = -1)\n",
    "scores_r2 = cross_val_score(regressor, X_train, Y_train, cv=skf, scoring = 'r2', n_jobs = -1)\n",
    "\n",
    "\n",
    "print(\"ECM Scores: {}\".format(scores_ecm))\n",
    "print('R2 Scores : {}'.format(scores_r2))\n",
    "print(np.mean(np.absolute(scores_ecm)))\n",
    "print(np.mean(np.absolute(scores_r2)))\n",
    "\n",
    "mse_results = []\n",
    "r2_results = []\n",
    "\n",
    "regressor = regressor.fit(X_train, Y_train)\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "\n",
    "cm = confusion_matrix(Y_test, y_pred)\n",
    "print(cm)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=regressor.classes_)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "precision=accuracy_score(Y_test, y_pred)\n",
    "print('Accuracy: {}'. format(precision))          \n",
    "\n",
    "sensibilidad=recall_score(Y_test, y_pred, average = 'weighted')\n",
    "print('Recall: {}'.format(sensibilidad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the high quantity of classes we have, the repressentation of the Multiple Logistic Regression model turns out to be unfeasible. This may be a limitation of our project. Besides, working on the balancing of our dataset, we may consider to diminish the amount of our classes to improve on this aspect.    \n",
    "\n",
    "When dealing with high dimensionality, we may try to improve it using univariant logistic regressions among features. Nonetheless, given the low correlation they have and the fact that we actually do not have to face with curse of dimensionality (number of samples >> 10 times number of features) we do not see necessary to diminish it a priori.\n",
    "\n",
    "Lastly, diminish the number of classes on this specific case may generate a higher uncertainty in relation to which type of cancer we are predicting. As a result, we may have to reach a trade-off between the number of classes and the generalization/uncertainty of type of cancer to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 596
    },
    "id": "cPmQO04e1aVQ",
    "outputId": "48b886fa-d938-42ad-d2f1-d33695cd5ee6"
   },
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(C = 20, solver='saga', penalty='l1', multi_class = 'multinomial', max_iter=2000, n_jobs = -1)\n",
    "\n",
    "regressor=logreg.fit(X_train, Y_train)\n",
    "\n",
    "y_pred_lasso = logreg.predict(X_test)\n",
    "scores_ecm = cross_val_score(logreg, X_train, Y_train, cv=skf, scoring = 'neg_mean_squared_error', n_jobs = -1)\n",
    "scores_r2 = cross_val_score(logreg, X_train, Y_train, cv=skf, scoring = 'r2', n_jobs = -1)\n",
    "\n",
    "\n",
    "print(\"ECM Scores: {}\".format(scores_ecm))\n",
    "print('R2 Scores : {}'.format(scores_r2))\n",
    "print(np.mean(np.absolute(scores_ecm)))\n",
    "print(np.mean(np.absolute(scores_r2)))\n",
    "\n",
    "cm = confusion_matrix(Y_test, y_pred_lasso)\n",
    "print(cm)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=logreg.classes_)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "\n",
    "precision=accuracy_score(Y_test, y_pred_lasso)\n",
    "print('Accuracy: {}'.format(precision))\n",
    "              \n",
    "sensibilidad_lasso=recall_score(Y_test, y_pred_lasso, average = 'weighted')\n",
    "print('Recall: {}'.format(sensibilidad_lasso))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 596
    },
    "id": "AXTBpDDk4PGv",
    "outputId": "7e428acb-718e-4a3f-de83-0650f386b686"
   },
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(C = 10, solver='saga', penalty='l2', multi_class = 'multinomial', max_iter=2000, n_jobs = -1)\n",
    "\n",
    "regressor=logreg.fit(X_train, Y_train)\n",
    "\n",
    "y_pred_ridge = logreg.predict(X_test)\n",
    "scores_ecm = cross_val_score(logreg, X_train, Y_train, cv=skf, scoring = 'neg_mean_squared_error', n_jobs = -1)\n",
    "scores_r2 = cross_val_score(logreg, X_train, Y_train, cv=skf, scoring = 'r2', n_jobs = -1)\n",
    "\n",
    "\n",
    "print(\"ECM Scores: {}\".format(scores_ecm))\n",
    "print('R2 Scores : {}'.format(scores_r2))\n",
    "print(np.mean(np.absolute(scores_ecm)))\n",
    "print(np.mean(np.absolute(scores_r2)))\n",
    "\n",
    "cm = confusion_matrix(Y_test, y_pred_ridge)\n",
    "print(cm)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=logreg.classes_)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "\n",
    "precision=accuracy_score(Y_test, y_pred_ridge)\n",
    "print('Accuracy: {}'.format(precision))\n",
    "\n",
    "sensibilidad_ridge=recall_score(Y_test, y_pred_ridge, average = 'weighted')\n",
    "print('Recall: {}'.format(sensibilidad_ridge))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 596
    },
    "id": "XDdqaGEH7b6T",
    "outputId": "221d14c9-ccc2-4229-bbbb-f412ce6e60a7"
   },
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(C = 10, solver='saga', penalty='elasticnet', multi_class = 'multinomial', max_iter=2000, n_jobs = -1, l1_ratio = 0.15)\n",
    "\n",
    "\n",
    "regressor=logreg.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "y_pred_en = logreg.predict(X_test)\n",
    "scores_ecm = cross_val_score(logreg, X_train, Y_train, cv=skf, scoring = 'neg_mean_squared_error', n_jobs = -1)\n",
    "scores_r2 = cross_val_score(logreg, X_train, Y_train, cv=skf, scoring = 'r2', n_jobs = -1)\n",
    "\n",
    "\n",
    "print(\"MSE Scores: {}\".format(scores_ecm))\n",
    "print('R2 Scores : {}'.format(scores_r2))\n",
    "print(np.mean(np.absolute(scores_ecm)))\n",
    "print(np.mean(np.absolute(scores_r2)))\n",
    "\n",
    "\n",
    "cm = confusion_matrix(Y_test, y_pred_en)\n",
    "print(cm)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=logreg.classes_)\n",
    "disp.plot()\n",
    "plt.show()\n",
    "\n",
    "precision=accuracy_score(Y_test, y_pred_en)\n",
    "print('Accuracy: {}'.format(precision))\n",
    "            \n",
    "#Calculamos la sensibilidad\n",
    "sensibilidad_en=recall_score(Y_test, y_pred_en, average = 'weighted')\n",
    "print('Recall: {}'.format(sensibilidad_en))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
